{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/Demo_TextPrep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la9AOHZv_-F3"
      },
      "source": [
        "# Demonstrations of Tokenisation, Stemming, Tagging and Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq3raemF_9hG"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# import all the resources for Natural Language Processing with Python\n",
        "nltk.download(\"book\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0tR2zVyAPw-"
      },
      "source": [
        "##Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EDxpyk8ALl_"
      },
      "source": [
        "sentence = \"\"\"At eight o'clock on Thursday morning\n",
        "Arthur didn't feel very good.\"\"\"\n",
        "\n",
        "#sentence=input('Enter sentence for tokenisation: ')\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kmb4nWUfbwR"
      },
      "source": [
        "##Stop Word Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR_dnFe6fgHp"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for w in tokens:\n",
        "    if w.lower() not in stop_words:\n",
        "        filtered_tokens.append(w)\n",
        "\n",
        "print(tokens)\n",
        "print(filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOGOiD4DATGO"
      },
      "source": [
        "##Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_peOs3LAWCn"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sentence = \"gaming, the gamers play games\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for word in words:\n",
        "    print(word + \":\" + ps.stem(word))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqhD1iz-A6f_"
      },
      "source": [
        "##Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VWYoJLbA51I"
      },
      "source": [
        "sentence = \"\"\"At eight o'clock on Thursday morning\n",
        "Arthur didn't feel very good.\"\"\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "print(tagged)\n",
        "\n",
        "entities = nltk.chunk.ne_chunk(tagged)\n",
        "\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD9CPiVqA8kY"
      },
      "source": [
        "##Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT0XpXJEA-Pg"
      },
      "source": [
        "nltk.download('omw-1.4')\n",
        "\n",
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
        "\n",
        "# \"a\" denotes adjective in part-of-speech\n",
        "#print(\"better :\", lemmatizer.lemmatize(\"better\", pos='n'))\n",
        "print(\"better :\", lemmatizer.lemmatize(\"better\", pos='a'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVozzZ_cB38Y"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "\n",
        "# map Penn Treebank tag set to Wordnet tags\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "\n",
        "sentence = \"Another way of achieving this task\"\n",
        "#sentence = input('Enter sentence for lemmatizer: ')\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for token, tag in pos_tag(tokens):\n",
        "    lemma = lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
        "    print(token, \"(\",tag,\") =>\", lemma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pQ_m6ARJnz"
      },
      "source": [
        "## Build a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQFZV_JKRMkY"
      },
      "source": [
        "text = nltk.corpus.gutenberg.raw('shakespeare-caesar.txt')\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMaih-SIbcKv"
      },
      "source": [
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"text size=\", len(tokens))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTtxvMn0bkiO"
      },
      "source": [
        "dictionary = {\"NUL\": 0, \"OOV\": 1}\n",
        "index = len(dictionary)\n",
        "for word in tokens:\n",
        "  if not word.lower() in dictionary:\n",
        "    dictionary[word.lower()] = index\n",
        "    index += 1\n",
        "\n",
        "print(\"dictionary size=\", len(dictionary))\n",
        "\n",
        "print(list(dictionary.items())[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjugeXJ0TVuc"
      },
      "source": [
        "reverse_dictionary = {}\n",
        "for key,val in dictionary.items():\n",
        "  reverse_dictionary[val] = key\n",
        "\n",
        "print(list(reverse_dictionary.items())[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4sH7R7exSo"
      },
      "source": [
        "## Convert to one-hot coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx1zYgZVkMjN"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "word_list = [ 1 ,0, 3 ,5 ]\n",
        "print(to_categorical(word_list))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}