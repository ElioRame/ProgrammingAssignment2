{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/Copy_of_FinalAssignmentPALS0039_help1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn5DPIaA8HGI",
        "outputId": "18f9b42c-e60a-4167-c2bb-8dc75b5ee54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe Project Gutenberg eBook of The Adventures of Tom Sawyer, Complete\\n', '    \\n', 'This ebook is for the use of anyone anywhere in the United States and\\n', 'most other parts of the world at no cost and with almost no restrictions\\n', 'whatsoever. You may copy it, give it away or re-use it under the terms\\n', 'of the Project Gutenberg License included with this ebook or online\\n', 'at www.gutenberg.org. If you are not located in the United States,\\n', 'you will have to check the laws of the country where you are located\\n', 'before using this eBook.\\n', '\\n', 'Title: The Adventures of Tom Sawyer, Complete\\n', '\\n', 'Author: Mark Twain\\n', '\\n', 'Release date: July 1, 2004 [eBook #74]\\n', '                Most recently updated: August 9, 2023\\n', '\\n', 'Language: English\\n', '\\n', 'Credits: David Widger\\n', '\\n', '\\n', '*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF TOM SAWYER, COMPLETE ***\\n', '\\n', '\\n', '\\n', '\\n', 'THE ADVENTURES OF TOM SAWYER\\n', '\\n', '\\n', 'By Mark Twain\\n', '\\n', '(Samuel Langhorne Clemens)\\n', '\\n', '\\n', '\\n', '\\n', 'CONTENTS\\n', '\\n', '\\n', 'CHAPTER I. Y-o-u-u Tom—Aunt Polly Decides Upon her Duty—Tom Practices\\n', 'Music—The Challenge—A Private Entrance\\n', '\\n', 'CHAPTER II. Strong Temptations—Strategic Movements—The Innocents\\n', 'Beguiled\\n', '\\n', 'CHAPTER III. Tom as a General—Triumph and Reward—Dismal\\n', 'Felicity—Commission and Omission\\n', '\\n', 'CHAPTER IV. Mental Acrobatics—Attending Sunday—School—The\\n']\n"
          ]
        }
      ],
      "source": [
        "# All seven books are used for the assignments, they are downloaded using read.lines() from library and renamed, they are then added to create a new string named 'data'\n",
        "with open(\"/content/pg74.txt\") as TomSawyer:\n",
        "  Book1 = TomSawyer.readlines()\n",
        "with open(\"/content/pg84.txt\") as Frankenstein:\n",
        "  Book2 = Frankenstein.readlines()\n",
        "with open(\"/content/pg345.txt\") as Dracula:\n",
        "  Book3 = Dracula.readlines()\n",
        "with open(\"/content/pg1342.txt\") as PrideandPrejudice:\n",
        "  Book4 = PrideandPrejudice.readlines()\n",
        "with open(\"/content/pg1727.txt\") as Odissey:\n",
        "  Book5 = Odissey.readlines()\n",
        "with open(\"/content/pg2701.txt\") as MobyDick:\n",
        "  Book6 = MobyDick.readlines()\n",
        "with open(\"/content/pg3207.txt\") as Leviathan:\n",
        "  Book7 = Leviathan.readlines()\n",
        "\n",
        "data = Book1 + Book2 + Book3 + Book4 + Book5 + Book6 + Book7\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-T2aK-DX8UO",
        "outputId": "503ee699-c09e-4a9a-db24-327c1aa742c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install -U nltk>=3.7.0\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import pad_sequence, ngrams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.models import Laplace\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import regex as re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LSTM, TimeDistributed, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer()\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1vaRZJYI2f",
        "outputId": "26f226c0-b685-44fe-b1f4-8a2c4c6ac685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of corpus: 5737483\n",
            "Length of vocabulary: 31\n",
            "Unique words in corpus: 43\n",
            "['<s>', '', 't', 'h', 'e', 'p', 'r', 'o', 'j', 'c', 'g', 'u', 'n', 'b', 'k', 'f', 'a', 'd', 'v', 's', 'm', 'w', 'y', 'l', '</s>', 'i', 'z', 'x', 'q', 'æ', '<UNK>']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import collections\n",
        "import operator\n",
        "import unicodedata\n",
        "# Creat a list to append the padded and processed text\n",
        "letters = []\n",
        "\n",
        "# Eliminate non-English characters and punctuation by first defining a function that identifies using booleans\n",
        "# whether the items in the string are alphabetical and part of the list of items in the unicodedata function\n",
        "# Unicode gives access to list of characters through specifications (in this case 'LATIN' and 'COMMON')\n",
        "# Therefore it identifies as True items that are part of the latin alphabet\n",
        "def is_english_letter(letters):\n",
        "    return letters.isalpha() and unicodedata.name(letters).startswith(('LATIN', 'COMMON'))\n",
        "\n",
        "# if the item is part of the latin alphabet it is added to a new list (output_list)\n",
        "def remove_non_english_letters(list_of_letters):\n",
        "    output_list = []\n",
        "    for letter in list_of_letters:\n",
        "        filtered = filter(is_english_letter, list(letter))\n",
        "        english_string = ''.join(filtered)\n",
        "        output_list.append(english_string)\n",
        "    return output_list\n",
        "\n",
        "# Texts are split into sentences using the newline (/n) symbol\n",
        "for text in data:\n",
        "  split_text = text.splitlines()\n",
        "  for sentence in split_text:\n",
        "   # The following code creates a new list. All letters in the text are first made lowercase and then appended\n",
        "   lower_text = [letter.lower() for letter in sentence]\n",
        "   # All letters that don't belong to the English alphabet are removed\n",
        "   eng_text = remove_non_english_letters(lower_text)\n",
        "   # The corpus is padded at the sentence level by using <s> at the start, and </s> at the end, of every sentence\n",
        "   padded_corpus = list(pad_sequence(eng_text, n=2, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"))\n",
        "   # The padded corpus is appended to the list \"letters\"\n",
        "   letters.append(padded_corpus)\n",
        "\n",
        "\n",
        "#for word in padded_words:\n",
        "   #tokenizer = RegexpTokenizer(r'\\w+')\n",
        "   #tokenised_word = tokenizer.tokenize(words)\n",
        "    #eng_letters = remove_non_english_letters(words)\n",
        "    #for word in eng_letters:\n",
        "     #for letter in word:\n",
        "      #letters.append(letter)\n",
        "\n",
        "#padded_corpus = list(pad_sequence(letters, n=2, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"))\n",
        "#for word in padded_words:\n",
        "  #if word in padded_words == \"<s>\" or \"</s>\":\n",
        "    #letters.append(word)\n",
        "  #else:\n",
        "    #for letter in words:\n",
        "      #letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "   #for punct in string.punctuation:\n",
        "    #letters = list(map(lambda x: \" \" if x == punct else x, tokenised_word))\n",
        "     #for punct in string.punctuation:\n",
        "      #text_punct = list(map(lambda x: \" \" if x == punct else x, letter))\n",
        "      #letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "#def tokenize(text):\n",
        "  #text_lower = [word.lower() for word in str(text)]\n",
        "  #for punct in string.punctuation:\n",
        "    #text_punct = list(map(lambda x: \" \" if x == punct else x, text_lower))\n",
        "  #return text_punct\n",
        "\n",
        "#tokenized_data = tokenize(data)\n",
        "#for words in tokenized_data:\n",
        "  #for letter in words:\n",
        "    #letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# The corpus is flattened to create a single list with all padded sentences\n",
        "corpus = list(flatten(letters))\n",
        "\n",
        "\n",
        "\n",
        "# A vocabulary is created using nltk.vocab which gives us a list of all the letters that appear in the text for 50 times or more\n",
        "vocab = Vocabulary(corpus, unk_cutoff=50)\n",
        "\n",
        "# Data from the corpus is partitioned into train and test sets\n",
        "test_letters = corpus[:100000]\n",
        "train_letters = corpus[100000:]\n",
        "\n",
        "# Inspection shows the presence of 31 vocab words, consistent with the English alphabet with added '<UNK>' and padding, as well as empty spaces between words\n",
        "print(\"Length of corpus:\", len(corpus))\n",
        "print(\"Length of vocabulary:\", len(vocab))\n",
        "print(\"Unique words in corpus:\", len(set(corpus)))\n",
        "\n",
        "show_vocab = [i for i in vocab]\n",
        "print(show_vocab)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N55ekRQ7Ye72",
        "outputId": "fb57888e-3e57-43c4-f82a-6a67eefc7210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 19, 23, 16, 2, 3, 4, 6, 19, 1, 7, 15, 1, 20, 7, 12, 4, 22, 1, 20, 7, 19, 2, 1]\n",
            "[0, 1, 2, 3, 4, 1, 5, 6, 7, 8, 4, 9, 2, 1, 10, 11, 2, 4, 12, 13, 4, 6, 10, 1, 4]\n"
          ]
        }
      ],
      "source": [
        "#Create a dictionary that maps letters to their indexes within the vocab. We iterate though the words in our vocabulary and assign an index to each one\n",
        "#The index then becomes the key value to our letter\n",
        "letter_to_index = {l: i for i, l in enumerate(vocab)}\n",
        "# i.e. if vocab = abcdefg\n",
        "# word_to_index = a : 1, b: 2, c:3, ...\n",
        "# The function maps data to index in the vocab so that the word 'cab' = [3, 1, 2]\n",
        "\n",
        "\n",
        "# We iterate through all the letters in the training and test sets and convert each letter to the index using the letter_to_index dictionary and the lookup function\n",
        "# which will return the correct index for each letter\n",
        "train_indices = [letter_to_index[vocab.lookup(l)] for l in train_letters]\n",
        "test_indices = [letter_to_index[vocab.lookup(l)] for l in test_letters]\n",
        "\n",
        "# A list of indexes can then be fed to the neural network\n",
        "print(train_indices[:25])\n",
        "print(test_indices[:25])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45gOwWriYuc-",
        "outputId": "9e9f9763-dd29-420d-ab95-8364660589de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11274, 500) (11274, 500)\n",
            "(200, 500) (200, 500)\n"
          ]
        }
      ],
      "source": [
        "seqlen = 500\n",
        "\n",
        "def prepare_sequences(letters, unk_index, seqlen=seqlen):\n",
        "  trunc_length = len(letters) % seqlen\n",
        "  X = np.array(letters)[trunc_length:].reshape((-1, seqlen))\n",
        "  y = np.array(letters + [unk_index])[trunc_length + 1:].reshape((-1, seqlen))\n",
        "  return X, y\n",
        "\n",
        "Xtrain, ytrain = prepare_sequences(train_indices, letter_to_index[\"<UNK>\"])\n",
        "Xtest, ytest = prepare_sequences(test_indices, letter_to_index[\"<UNK>\"])\n",
        "\n",
        "#Inspect\n",
        "print(Xtrain.shape, ytrain.shape)\n",
        "print(Xtest.shape, ytest.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "DMauwpOHY2Ff",
        "outputId": "07183d55-29b7-4715-906b-c41aeddf729b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_5              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "  cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "  return perplexity\n",
        "\n",
        "def build_model(input_width=len(vocab), input_length=seqlen, output_width=len(vocab), learning_rate=0.01):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=input_width, output_dim=64, input_length=input_length))\n",
        "  model.add(LSTM(32, dropout=0.2, return_sequences=True, activation='tanh'))\n",
        "  model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
        "  model.add(TimeDistributed(Dense(output_width, activation='softmax')));\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=[perplexity])\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "#Inspect\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfrg_QKHY6lh",
        "outputId": "68147a23-06d3-4a91-a9e7-2d96918171cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - loss: 2.6784 - perplexity: 15.5088 - val_loss: 2.0698 - val_perplexity: 7.9539\n",
            "Epoch 2/100\n",
            "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 2s/step - loss: 2.0548 - perplexity: 7.8126 - val_loss: 1.9057 - val_perplexity: 6.7508\n",
            "Epoch 3/100\n",
            "\u001b[1m120/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m55s\u001b[0m 1s/step - loss: 1.9210 - perplexity: 6.8285"
          ]
        }
      ],
      "source": [
        "# model.fit is used on the training data.\n",
        "# Ten percent of the training data is used for validation - the remaining amount is plit into 121 batches of 64 sequences each, which will be trained for 200 epochs.\n",
        "# EarlyStopping is part of the tensorflow package, it will monitor a specific value (in this case, 'val_loss'), and it will stop if there has been no meaningful change for five epochs\n",
        "# This helps to avoid overfitting\n",
        "training_model = model.fit(Xtrain,ytrain, batch_size=64, validation_split=0.10, epochs=100, callbacks=EarlyStopping(monitor=\"val_loss\", patience=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_YSlhYIbQSQ"
      },
      "outputs": [],
      "source": [
        "# Make two graphs, plotting respectively the Loss and Perplexity values\n",
        "\n",
        "def plot_training_history(training_model):\n",
        "  # Choose the number of plots, how they should appear (side to side), and the size of the final figure\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  # Plot 1: Loss - plot a line graph for both the training and validation set values throughout the different epochs ('loss' and 'val_loss') in training_info\n",
        "  axs[0].plot(training_model.history['loss'], label=\"training set\")\n",
        "  axs[0].plot(training_model.history['val_loss'], label=\"validation set\")\n",
        "  axs[0].set_xlabel(\"Epoch\")\n",
        "  axs[0].set_ylabel(\"Loss\")\n",
        "  axs[0].grid(True)\n",
        "  axs[0].legend()\n",
        "  try:\n",
        "    # Plot 2: Perplexity - plot a line graph for both the training and validation set values throughout the different epochs\n",
        "    axs[1].plot(training_model.history['perplexity'], label=\"training set\")\n",
        "    axs[1].plot(training_model.history['val_perplexity'], label=\"validation set\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Perplexity\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "  except:\n",
        "    pass\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(training_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzpCeOiBbVDo"
      },
      "outputs": [],
      "source": [
        "# Calculate loss and perplexity. Both results are ..., indicating that ...\n",
        "\n",
        "test_loss, test_perplexity = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt6uPTLebal5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Map indexes stored through word_to_index() back to their original form\n",
        "index_to_letter = {i: l for l, i in letter_to_index.items()}\n",
        "\n",
        "# Choose seed text\n",
        "seed_text = \"merry christ\"\n",
        "# Choose how many characters you want the model to predict\n",
        "next_letters = 3\n",
        "#Create a list to add the tokenised and padded seed_text\n",
        "sequence = []\n",
        "\n",
        "#Lower, tokenise, and remove punctuation from seed_text\n",
        "for text in seed_text:\n",
        "  lower_t = [word.lower() for word in seed_text]\n",
        "  for words in lower_t:\n",
        "   tokenizer = RegexpTokenizer(r'\\w+')\n",
        "   token_word = tokenizer.tokenize(words)\n",
        "   for word in token_word:\n",
        "    for letter in word:\n",
        "      sequence.append(letter_to_index[letter])\n",
        "\n",
        "# Pad the sequence\n",
        "padded_seq = pad_sequences([sequence], maxlen = len(sequence), padding = \"post\")\n",
        "\n",
        "# Use model.predict to estimate how likely it is for each letter in the vocabulary to come after the seed_text\n",
        "for i in range(next_letters):\n",
        "  full_prediction = model.predict(padded_seq, verbose=0)\n",
        "  next_three_letters = full_prediction[0][len(padded_seq) + i]\n",
        "  # Use np.argmax to obtain the letter with the highest probability amongst the vocabulary items\n",
        "  prediction = index_to_letter[np.argmax(next_three_letters)]\n",
        "  # Add each prediction to the seed_text\n",
        "  seed_text += prediction\n",
        "\n",
        "\n",
        "# Print sequence\n",
        "print(\"Next_three_letters:\", seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBjuF5HY6AL3"
      },
      "source": [
        "The present notebook makes use of six out of the seven texts made available for this assignment; namely: \"The Adventures of Tom Sawyer\" (Book 1), \"Frankenstein\" (Book 2), \"Dracula\" (Book 3), \"Pride and Prejudice\"(Book 4), \"Moby Dick\" (Book 6), and \"Leviathan\" (Book 7). Given the nature of the assignment - which required to build a language model able to predict the next three letters in a given English seed text - \"The Odissey\" (Book 5) was excluded because of the presence of Ancient Greek alphabet. While the complete exclusion of the text can be justified, a better result would have been achievable by the removal of letters that are not part of the English alphabet from the tokenized text during data preparation; this would have made it possible to add more data to the corpus while training the model on English sequences only. However, because the other texts provided a corpus of length 3845425, the data provided by the other six books was here considered enough to construct a meaningful model without the added presence of Book 5.\n",
        "\n",
        "The text was first tokenised at the word level, with punctuation, newlines, and spaces being excluded. A single flattened list was then created with each letter from the corpus forming a separate item. The sequence was padded, so as to inform the model of the start and end of the corpus. A vocabulary was created using the NLTK library with a cut-off argument of 50. This constructed the input of the language model by creating an object that contained every letter that was present more than 50 times thoughout the corpus. A high cut-off frequency was chosen so as to exclude other elements from the text as the trainable input, such as numbers and unknown, without excluding them from the corpus. An index was then created so as to obtain a numerical equivalent of the items in the list based on their position in the vocabulary. The creation of this index was ... . Analysis of the corpus revealed a vocabulary set of ..., while the total length of unique letters in the text is ... .Finally the list was separated into a training set, which comprised all the letters in the corpus apart of the first 10000, which were selected to be used for the test set.\n",
        "\n",
        "7.2\n",
        "work heavily with nltk\n",
        "we use several nltk classes such as vocabulary and utility functions such as pad_sequences\n",
        "neural networks are taken from tensorflow, adam the optimizer, time distributed is a layer that connects structure from a feedforward network to the lstm\n",
        "\n",
        "padding to be able to distinguish between sentences\n",
        "create a vocabulary giving cutoff as a parameters\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UGmaej6DpKxZ-YXKPbCP7CfKyFrCgDea",
      "authorship_tag": "ABX9TyPqRFydtOBd4a9MimnqYZpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}