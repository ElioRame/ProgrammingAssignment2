{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/Another_copy_of_FinalAssignmentPALS0039_finalfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn5DPIaA8HGI",
        "outputId": "552056f5-100f-4a49-a597-b24d135e6ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe Project Gutenberg eBook of The Adventures of Tom Sawyer, Complete\\n', '    \\n', 'This ebook is for the use of anyone anywhere in the United States and\\n', 'most other parts of the world at no cost and with almost no restrictions\\n', 'whatsoever. You may copy it, give it away or re-use it under the terms\\n', 'of the Project Gutenberg License included with this ebook or online\\n', 'at www.gutenberg.org. If you are not located in the United States,\\n', 'you will have to check the laws of the country where you are located\\n', 'before using this eBook.\\n', '\\n', 'Title: The Adventures of Tom Sawyer, Complete\\n', '\\n', 'Author: Mark Twain\\n', '\\n', 'Release date: July 1, 2004 [eBook #74]\\n', '                Most recently updated: August 9, 2023\\n', '\\n', 'Language: English\\n', '\\n', 'Credits: David Widger\\n', '\\n', '\\n', '*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF TOM SAWYER, COMPLETE ***\\n', '\\n', '\\n', '\\n', '\\n', 'THE ADVENTURES OF TOM SAWYER\\n', '\\n', '\\n', 'By Mark Twain\\n', '\\n', '(Samuel Langhorne Clemens)\\n', '\\n', '\\n', '\\n', '\\n', 'CONTENTS\\n', '\\n', '\\n', 'CHAPTER I. Y-o-u-u Tom—Aunt Polly Decides Upon her Duty—Tom Practices\\n', 'Music—The Challenge—A Private Entrance\\n', '\\n', 'CHAPTER II. Strong Temptations—Strategic Movements—The Innocents\\n', 'Beguiled\\n', '\\n', 'CHAPTER III. Tom as a General—Triumph and Reward—Dismal\\n', 'Felicity—Commission and Omission\\n', '\\n', 'CHAPTER IV. Mental Acrobatics—Attending Sunday—School—The\\n']\n"
          ]
        }
      ],
      "source": [
        "# All seven books are used for the assignments, they are downloaded using read.lines() from library and renamed\n",
        "with open(\"/content/pg74.txt\") as TomSawyer:\n",
        "  Book1 = TomSawyer.readlines()\n",
        "with open(\"/content/pg84.txt\") as Frankenstein:\n",
        "  Book2 = Frankenstein.readlines()\n",
        "with open(\"/content/pg345.txt\") as Dracula:\n",
        "  Book3 = Dracula.readlines()\n",
        "with open(\"/content/pg1342.txt\") as PrideandPrejudice:\n",
        "  Book4 = PrideandPrejudice.readlines()\n",
        "with open(\"/content/pg1727.txt\") as Odissey:\n",
        "  Book5 = Odissey.readlines()\n",
        "with open(\"/content/pg2701.txt\") as MobyDick:\n",
        "  Book6 = MobyDick.readlines()\n",
        "with open(\"/content/pg3207.txt\") as Leviathan:\n",
        "  Book7 = Leviathan.readlines()\n",
        "\n",
        "# They are then added together to create a new list named 'data'\n",
        "data = Book1 + Book2 + Book3 + Book4 + Book5 + Book6 + Book7\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-T2aK-DX8UO",
        "outputId": "e2e90105-3dc2-499f-ec86-60658269f0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install -U nltk>=3.7.0\n",
        "\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import pad_sequence, ngrams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.lm.preprocessing import flatten\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import regex as re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LSTM, TimeDistributed, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "token = Tokenizer()\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1vaRZJYI2f",
        "outputId": "7a295c0f-fd21-4bb6-fedc-b145f02ef993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are all letters in the raw text part of the Latin alphabet? False\n",
            "Non-Latin letters in raw text: {'α', '4', '*', '&', '—', 'φ', 'ά', '\\n', 'ὺ', 'ῥ', 'ύ', '^', 'σ', 'ῶ', 'γ', 'έ', '‘', 'ῖ', '!', 'ἷ', 'ὶ', 'ῇ', 'ί', '“', 'ρ', '7', 'τ', 'Ν', 'ו', '0', 'ε', ',', '•', 'ἰ', 'ὀ', '}', ' ', 'ι', 'ν', 'ζ', '·', 'ὄ', '?', '.', '-', '3', '1', 'π', '(', '2', '#', 'ἠ', ']', '”', 'ὼ', 'ἄ', 'χ', '/', '’', 'ς', 'ο', '_', 'ώ', 'λ', 'Ἰ', 'ό', '%', '6', 'ἔ', 'ἐ', '[', ')', 'μ', 'Μ', '{', ';', ':', 'ח', '£', 'η', '9', 'κ', 'ῆ', 'ἦ', 'δ', '8', 'ή', '5', 'ὡ', 'ϰ', 'θ', '\"', '$', 'υ', '\\ufeff', 'ἀ', 'ῳ', 'ὦ', 'ω', '™'}\n",
            "Non-Latin letters in processed text: {'<s>', '', '</s>'} \n",
            "\n",
            "Inspect corpus: ['<s>', '', 't', 'h', 'e', '', 'p', 'r', 'o', 'j', 'e', 'c', 't', '', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', '', 'e', 'b', 'o', 'o', 'k', '', 'o', 'f', '', 't', 'h', 'e', '', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', '', 'o', 'f', '', 't', 'o', 'm', '', 's', 'a', 'w', 'y', 'e', 'r', '', '', 'c', 'o', 'm', 'p', 'l', 'e', 't', 'e', '</s>', '<s>', '', '', '', '', '</s>', '<s>', 't', 'h', 'i', 's', '', 'e', 'b', 'o', 'o', 'k', '', 'i', 's', '', 'f', 'o', 'r', '', 't', 'h', 'e', '', 'u', 's', 'e', '', 'o', 'f', '', 'a', 'n', 'y', 'o', 'n', 'e', '', 'a', 'n', 'y', 'w', 'h', 'e', 'r', 'e', '', 'i', 'n', '', 't', 'h', 'e', '', 'u', 'n', 'i', 't', 'e', 'd', '', 's', 't', 'a', 't', 'e', 's', '', 'a', 'n', 'd', '</s>', '<s>', 'm', 'o', 's', 't', '', 'o', 't', 'h', 'e', 'r', '', 'p', 'a', 'r', 't', 's', '', 'o', 'f', '', 't', 'h', 'e', '', 'w', 'o', 'r', 'l', 'd', '', 'a', 't', '', 'n', 'o', '', 'c', 'o', 's', 't', '', 'a', 'n', 'd', '', 'w', 'i', 't', 'h', '']\n",
            "Length of corpus: 5737483 \n",
            "\n",
            "Unique letters: {'', 'œ', 'á', 'l', 'e', 't', 'g', 'ï', 'í', 'i', 'h', '<s>', 'y', 'x', 'é', 'a', 'è', 'ô', 'd', 'w', '</s>', 'à', 'ê', 'b', 'ö', 'n', 'ë', 'm', 'ù', 'j', 'c', 'æ', 'v', 's', 'o', 'z', 'f', 'q', 'â', 'u', 'p', 'k', 'r'}\n",
            "Unique letters in corpus: 43 \n",
            "\n",
            "Vocabulary: ['<s>', '', 't', 'h', 'e', 'p', 'r', 'o', 'j', 'c', 'g', 'u', 'n', 'b', 'k', 'f', 'a', 'd', 'v', 's', 'm', 'w', 'y', 'l', '</s>', 'i', 'z', 'x', 'q', 'æ', '<UNK>']\n",
            "Length of vocabulary: 31 \n",
            "\n",
            "Discarded letters: ['œ', 'á', 'ï', 'í', 'é', 'è', 'ô', 'à', 'ê', 'ö', 'ë', 'ù', 'â']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import collections\n",
        "import operator\n",
        "import unicodedata\n",
        "\n",
        "# Check whether non-Latin alphabet characters are present in the data\n",
        "# Eliminate non-English characters and punctuation by first defining a function that identifies, using booleans, whether\n",
        "# all characters are alphabetical and part of the Latin alphabet script in the unicodedata function\n",
        "# This function also preserves common occurring symbols that are not part of the Latin alphabet (i.e. currency)\n",
        "# Unicode gives access to list of characters through specifications (in this case 'LATIN' and 'COMMON')\n",
        "# Therefore it identifies as True items that are part of the latin alphabet\n",
        "def is_english_letter(letters):\n",
        "  return str(letters).isalpha() and unicodedata.name(letters).startswith(('LATIN', 'COMMON'))\n",
        "\n",
        "# Check if present data contains only letters that are part of the Latin alphabet\n",
        "unwanted_letters = []\n",
        "for sentence in data:\n",
        "  for letter in sentence:\n",
        "   if is_english_letter(letter) == False:\n",
        "    unwanted_letters.append(letter)\n",
        "print(\"Are all letters in the raw text part of the Latin alphabet?\", is_english_letter(data))\n",
        "print(\"Non-Latin letters in raw text:\", set(unwanted_letters))\n",
        "\n",
        "# There are characters belonging to different alphabets\n",
        "# Process the dataset to eliminate unwanted characters\n",
        "# if the item is part of the latin alphabet it is added to a new list\n",
        "def remove_non_english_letters(data):\n",
        "    english_list = []\n",
        "    for letter in data:\n",
        "        filtered = filter(is_english_letter, list(letter))\n",
        "        english_string = \"\".join(filtered)\n",
        "        english_list.append(english_string)\n",
        "\n",
        "    return english_list\n",
        "\n",
        "# Create a list to append the padded and processed text\n",
        "letters = []\n",
        "\n",
        "# Texts are stripped of empty strings and spaces and split into a list of lists\n",
        "# Every item in the list is a sentence in the text, split at every newline\n",
        "for text in data:\n",
        "  split_text = text.splitlines()\n",
        "  for sentence in split_text:\n",
        "   # The following code creates a new list. For all the sentences in the text, all letters are made lowercase\n",
        "   # This creates a new list of lists with every letter in every list forming a separate token\n",
        "     lower_text = [letter.lower() for letter in sentence]\n",
        "   # All letters that don't belong to the English alphabet are removed\n",
        "     eng_text = remove_non_english_letters(lower_text)\n",
        "   # The corpus is padded at the sentence level by using <s> at the start, and </s> at the end, of every sentence\n",
        "     padded_corpus = list(pad_sequence(eng_text, n=2, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"))\n",
        "   # The padded corpus is appended to the list \"letters\"\n",
        "     letters.append(padded_corpus)\n",
        "\n",
        "\n",
        "# The corpus is flattened to create a single list with all padded sentences\n",
        "corpus = list(flatten(letters))\n",
        "\n",
        "\n",
        "# A vocabulary is created using nltk.vocab which gives us a list of all the letters that appear in the text for 50 times or more\n",
        "vocab = Vocabulary(corpus, unk_cutoff=50)\n",
        "\n",
        "# Data from the corpus is partitioned into train and test sets\n",
        "test_letters = corpus[:100000]\n",
        "train_letters = corpus[100000:]\n",
        "\n",
        "# Check if foreign letters were successfully excluded\n",
        "latin_false  = []\n",
        "for i in corpus:\n",
        "  if is_english_letter(i) == False:\n",
        "    latin_false.append(i)\n",
        "print(\"Non-Latin letters in processed text:\", set(latin_false), \"\\n\")\n",
        "\n",
        "# Inspection shows the presence of 31 vocab words, consistent with the English alphabet with added '<UNK>' and padding, as well as empty spaces between words\n",
        "print(\"Inspect corpus:\", corpus[:200])\n",
        "print(\"Length of corpus:\", len(corpus), \"\\n\")\n",
        "\n",
        "print(\"Unique letters:\", set(corpus))\n",
        "print(\"Unique letters in corpus:\", len(set(corpus)), \"\\n\")\n",
        "\n",
        "show_vocab = [i for i in vocab]\n",
        "print(\"Vocabulary:\", show_vocab)\n",
        "print(\"Length of vocabulary:\", len(vocab), \"\\n\")\n",
        "\n",
        "for i in set(corpus):\n",
        "    discarded_letters = [i for i in set(corpus) if i not in vocab]\n",
        "print(\"Discarded letters:\", discarded_letters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N55ekRQ7Ye72",
        "outputId": "2fc9fc3d-c6d8-43fa-8ceb-1bbcaa81153b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 19, 23, 16, 2, 3, 4, 6, 19, 1, 7, 15, 1, 20, 7, 12, 4, 22, 1, 20, 7, 19, 2, 1]\n",
            "[0, 1, 2, 3, 4, 1, 5, 6, 7, 8, 4, 9, 2, 1, 10, 11, 2, 4, 12, 13, 4, 6, 10, 1, 4]\n"
          ]
        }
      ],
      "source": [
        "#Create a dictionary that maps letters to their indexes within the vocab. We iterate though the words in our vocabulary and assign an index to each one\n",
        "#The index then becomes the key value to our letter\n",
        "letter_to_index = {l: i for i, l in enumerate(vocab)}\n",
        "# i.e. if vocab = abcdefg\n",
        "# word_to_index = {a : 1, b : 2, c : 3, d : 4, e : 5, f : 6, g : 6}\n",
        "# The function maps data to index in the vocab so that the word 'cab' = [3, 1, 2]\n",
        "\n",
        "\n",
        "# We iterate through all the letters in the training and test sets and convert each letter to the index using the letter_to_index dictionary and the lookup function\n",
        "# which will return the correct index for each letter\n",
        "train_indices = [letter_to_index[vocab.lookup(l)] for l in train_letters]\n",
        "test_indices = [letter_to_index[vocab.lookup(l)] for l in test_letters]\n",
        "\n",
        "# A list of indexes can then be fed to the neural network\n",
        "print(train_indices[:25])\n",
        "print(test_indices[:25])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45gOwWriYuc-",
        "outputId": "19cf498c-d050-4b42-ea38-6cc8e49357bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70468, 80) (70468, 80)\n",
            "(1250, 80) (1250, 80)\n"
          ]
        }
      ],
      "source": [
        "# Choose sequence length\n",
        "seqlen = 80\n",
        "\n",
        "# we create a function which takes the input, an index value, and the sequence length as arguments\n",
        "# We create sequences of same length by using the modulo (%) which divides our input into 80 letter sequences while the remainder is discarded\n",
        "def prepare_sequences(letters, unk_index, seqlen=seqlen):\n",
        "  # We calculate the remainder of our corpus length divided by the chosen sequence length\n",
        "  trunc_length = len(letters) % seqlen\n",
        "  # We create a two dimentional array of shape (, 80) by taking away the trunc_length remainder from the start of the corpus\n",
        "  X = np.array(letters)[trunc_length:].reshape((-1, seqlen))\n",
        "  # Similarly, a (, 80) dimensional array is created for the corpus with the added unk_index by taking away the remainder from the start of the corpus\n",
        "  # The output is a shifted by 1, with unk_index representing the next letter to predict.\n",
        "  y = np.array(letters + [unk_index])[trunc_length + 1:].reshape((-1, seqlen))\n",
        "  return X, y\n",
        "\n",
        "# The unk_index is set to the vocabulary index for \"<UNK>\"\n",
        "Xtrain, ytrain = prepare_sequences(train_indices, letter_to_index[\"<UNK>\"])\n",
        "Xtest, ytest = prepare_sequences(test_indices, letter_to_index[\"<UNK>\"])\n",
        "\n",
        "\n",
        "print(Xtrain.shape, ytrain.shape)\n",
        "print(Xtest.shape, ytest.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DMauwpOHY2Ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "394d2920-a694-4deb-d021-6146a3b9bbcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_20 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_10             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_10             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# This function calculates the perplexity by taking the product of all predicted probabilities, take the geometric mean, and the inverse\n",
        "def perplexity(y_true, y_pred):\n",
        "  cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "  return perplexity\n",
        "\n",
        "#A sequential model is built with vocab length as input and output width, 80 as input length, and low learning rate\n",
        "def build_model(input_width=len(vocab), input_length=seqlen, output_width=len(vocab), learning_rate=0.01):\n",
        " model = Sequential()\n",
        "\n",
        "  # the Embedding layer transforms each input into a unique vector of length output_dim, its aim is dimensionality reduction\n",
        "  # Through the unique vectors the model learns relationships between the inputs\n",
        " model.add(Embedding(input_dim=input_width, output_dim=128, input_length=seqlen))\n",
        "\n",
        "  #The following LSTM model returns the hidden state output for each input time step,\n",
        " model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
        " model.add(Dropout(0.2))\n",
        " model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
        " model.add(TimeDistributed(Dense(30, activation='softmax')));\n",
        " model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=[perplexity])\n",
        " return model\n",
        "\n",
        "LSTMmodel = build_model()\n",
        "\n",
        "\n",
        "#example_model = model.build(input_shape=(128, 80))\n",
        "#Inspect\n",
        "\n",
        "print(LSTMmodel.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hfrg_QKHY6lh",
        "outputId": "248069fb-5db4-4324-dbde-fb67181ab147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 227ms/step - loss: 2.6761 - perplexity: 15.0615 - val_loss: 2.2751 - val_perplexity: 9.7367\n",
            "Epoch 2/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 196ms/step - loss: 2.3059 - perplexity: 10.0359 - val_loss: 2.1892 - val_perplexity: 8.9336\n",
            "Epoch 3/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 197ms/step - loss: 2.2409 - perplexity: 9.4030 - val_loss: 2.1561 - val_perplexity: 8.6433\n",
            "Epoch 4/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 209ms/step - loss: 2.2095 - perplexity: 9.1123 - val_loss: 2.1297 - val_perplexity: 8.4177\n",
            "Epoch 5/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 195ms/step - loss: 2.1870 - perplexity: 8.9096 - val_loss: 2.1104 - val_perplexity: 8.2562\n",
            "Epoch 6/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 195ms/step - loss: 2.1660 - perplexity: 8.7240 - val_loss: 2.0869 - val_perplexity: 8.0641\n",
            "Epoch 7/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 195ms/step - loss: 2.1526 - perplexity: 8.6079 - val_loss: 2.0794 - val_perplexity: 8.0040\n",
            "Epoch 8/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 209ms/step - loss: 2.1417 - perplexity: 8.5149 - val_loss: 2.0685 - val_perplexity: 7.9167\n",
            "Epoch 9/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 196ms/step - loss: 2.1326 - perplexity: 8.4379 - val_loss: 2.0640 - val_perplexity: 7.8805\n",
            "Epoch 10/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 193ms/step - loss: 2.1267 - perplexity: 8.3881 - val_loss: 2.0573 - val_perplexity: 7.8282\n",
            "Epoch 11/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 196ms/step - loss: 2.1213 - perplexity: 8.3427 - val_loss: 2.0531 - val_perplexity: 7.7954\n",
            "Epoch 12/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 206ms/step - loss: 2.1179 - perplexity: 8.3141 - val_loss: 2.0463 - val_perplexity: 7.7425\n",
            "Epoch 13/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 210ms/step - loss: 2.1143 - perplexity: 8.2847 - val_loss: 2.0414 - val_perplexity: 7.7043\n",
            "Epoch 14/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 196ms/step - loss: 2.1091 - perplexity: 8.2414 - val_loss: 2.0460 - val_perplexity: 7.7400\n",
            "Epoch 15/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 194ms/step - loss: 2.1071 - perplexity: 8.2246 - val_loss: 2.0394 - val_perplexity: 7.6887\n",
            "Epoch 16/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 211ms/step - loss: 2.1046 - perplexity: 8.2049 - val_loss: 2.0360 - val_perplexity: 7.6626\n",
            "Epoch 17/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 193ms/step - loss: 2.1031 - perplexity: 8.1927 - val_loss: 2.0343 - val_perplexity: 7.6498\n",
            "Epoch 18/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 197ms/step - loss: 2.1001 - perplexity: 8.1675 - val_loss: 2.0337 - val_perplexity: 7.6447\n",
            "Epoch 19/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 197ms/step - loss: 2.0976 - perplexity: 8.1474 - val_loss: 2.0284 - val_perplexity: 7.6042\n",
            "Epoch 20/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 194ms/step - loss: 2.0974 - perplexity: 8.1456 - val_loss: 2.0338 - val_perplexity: 7.6456\n",
            "Epoch 21/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 196ms/step - loss: 2.0958 - perplexity: 8.1327 - val_loss: 2.0333 - val_perplexity: 7.6416\n",
            "Epoch 22/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 196ms/step - loss: 2.0939 - perplexity: 8.1172 - val_loss: 2.0323 - val_perplexity: 7.6344\n",
            "Epoch 23/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 198ms/step - loss: 2.0916 - perplexity: 8.0985 - val_loss: 2.0276 - val_perplexity: 7.5984\n",
            "Epoch 24/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 197ms/step - loss: 2.0896 - perplexity: 8.0824 - val_loss: 2.0257 - val_perplexity: 7.5834\n",
            "Epoch 25/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 194ms/step - loss: 2.0882 - perplexity: 8.0716 - val_loss: 2.0263 - val_perplexity: 7.5882\n",
            "Epoch 26/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 196ms/step - loss: 2.0890 - perplexity: 8.0781 - val_loss: 2.0243 - val_perplexity: 7.5738\n",
            "Epoch 27/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 198ms/step - loss: 2.0872 - perplexity: 8.0631 - val_loss: 2.0241 - val_perplexity: 7.5715\n",
            "Epoch 28/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 197ms/step - loss: 2.0877 - perplexity: 8.0672 - val_loss: 2.0240 - val_perplexity: 7.5712\n",
            "Epoch 29/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 212ms/step - loss: 2.0836 - perplexity: 8.0340 - val_loss: 2.0231 - val_perplexity: 7.5640\n",
            "Epoch 30/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 194ms/step - loss: 2.0843 - perplexity: 8.0399 - val_loss: 2.0257 - val_perplexity: 7.5840\n",
            "Epoch 31/50\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 197ms/step - loss: 2.0843 - perplexity: 8.0397 - val_loss: 2.0248 - val_perplexity: 7.5767\n",
            "Epoch 32/50\n",
            "\u001b[1m 73/351\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 182ms/step - loss: 2.0828 - perplexity: 8.0277"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2d34817a9549>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# EarlyStopping is part of the tensorflow package, it will monitor a specific value (in this case, 'val_loss'), and it will stop if there has been no meaningful change for five epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This helps to avoid overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtraining_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# model.fit is used on the training data.\n",
        "# Ten percent of the training data is used for validation - the remaining amount is plit into 216 batches of 94 sequences each, which are trained for 100 epochs.\n",
        "# EarlyStopping is part of the tensorflow package, it will monitor a specific value (in this case, 'val_loss'), and it will stop if there has been no meaningful change for five epochs\n",
        "# This helps to avoid overfitting\n",
        "training_model = LSTMmodel.fit(Xtrain,ytrain, batch_size=128, validation_split=0.20, epochs=50, callbacks=EarlyStopping(monitor=\"val_loss\", patience=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_YSlhYIbQSQ"
      },
      "outputs": [],
      "source": [
        "# Make two graphs, plotting respectively the Loss and Perplexity values\n",
        "\n",
        "def plot_training_history(training_model):\n",
        "  # Choose the number of plots, how they should appear (side to side), and the size of the final figure\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  # Plot 1: Loss - plot a line graph for both the training and validation set values throughout the different epochs ('loss' and 'val_loss') in training_info\n",
        "  axs[0].plot(training_model.history['loss'], label=\"training dataset\")\n",
        "  axs[0].plot(training_model.history['val_loss'], label=\"validation dataset\")\n",
        "  axs[0].set_y_lim(1, 10)\n",
        "  axs[0].set_xlabel(\"Epoch\")\n",
        "  axs[0].set_ylabel(\"Loss\")\n",
        "  axs[0].legend()\n",
        "  # Plot 2: Perplexity - plot a line graph for both the training and validation set values throughout the different epochs\n",
        "  axs[1].plot(training_model.history['perplexity'], label=\"training dataset\")\n",
        "  axs[1].plot(training_model.history['val_perplexity'], label=\"validation dataset\")\n",
        "  axs[1].set_y_lim(0, 31)\n",
        "  axs[1].set_xlabel(\"Epoch\")\n",
        "  axs[1].set_ylabel(\"Perplexity\")\n",
        "  axs[1].legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(training_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzpCeOiBbVDo"
      },
      "outputs": [],
      "source": [
        "# Calculate loss and perplexity\n",
        "# 5.87 test perplexity and 1.77 test loss for Embedding, output_dim = 28), EarlyStopping stopped at epoch 38\n",
        "test_loss, test_perplexity = LSTMmodel.evaluate(Xtest, ytest, verbose=0)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt6uPTLebal5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Map indexes stored through word_to_index() back to their original form\n",
        "index_to_letter = {i: l for l, i in letter_to_index.items()}\n",
        "\n",
        "# Choose seed text\n",
        "seed_text = \"<s> Merry christ\"\n",
        "# Choose how many characters you want the model to predict\n",
        "next_letters = 3\n",
        "#Create a list to add the tokenised and padded seed_text\n",
        "sequence = []\n",
        "\n",
        "#Lower, tokenise, and remove punctuation from seed_text\n",
        "for text in seed_text:\n",
        "  lower_t = [word.lower() for word in seed_text]\n",
        "  for words in lower_t:\n",
        "   tokenizer = RegexpTokenizer(r'\\w+')\n",
        "   token_word = tokenizer.tokenize(words)\n",
        "   for word in token_word:\n",
        "    for letter in word:\n",
        "      sequence.append(letter_to_index[letter])\n",
        "\n",
        "# Pad the sequence\n",
        "padded_seq = pad_sequences([sequence], maxlen = len(sequence), padding = \"post\")\n",
        "\n",
        "# Use model.predict to estimate how likely it is for each letter in the vocabulary to come after the seed_text\n",
        "for i in range(next_letters):\n",
        "  full_prediction = LSTMmodel.predict(padded_seq, verbose=0)\n",
        "  next_three_letters = full_prediction[0][len(padded_seq) + i]\n",
        "  # Use np.argmax to obtain the letter with the highest probability amongst the vocabulary items\n",
        "  prediction = index_to_letter[np.argmax(next_three_letters)]\n",
        "  # Add each prediction to the seed_text\n",
        "  seed_text += prediction\n",
        "\n",
        "\n",
        "# Print sequence\n",
        "print(\"Next_three_letters:\", seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBjuF5HY6AL3"
      },
      "source": [
        "1.\tData pre-processing\n",
        "\n",
        "The present notebook makes use of all the seven texts made available for this assignment; namely: \"The Adventures of Tom Sawyer\" (Book 1), \"Frankenstein\" (Book 2), \"Dracula\" (Book 3), \"Pride and Prejudice\"(Book 4), \"The Odyssey\" (Book 5), \"Moby Dick\" (Book 6), and \"Leviathan\" (Book 7). The pre-processing of the data was facilitated by the use of several NLTK classes, such as the nltk.lm.Vocabulary and flatten functions, the nltk.util.pad_sequence utility function, and the nltk.tokenize.RegexpTokenizer function.\n",
        "Upon inspection of the dataset, it became clear that the corpus contained letters belonging to non-Latin alphabets. Given the nature of the assignment - which required to build a language model able to predict the next three letters in a given English seed text - the data was pre-processed to include only letters that are part of the Latin alphabet by creating a new list which only comprised tokens present in the unicodedata dataset under the “LATIN” and “COMMON” code. The text was first stripped of empty strings and tokenised at the newline level. After eliminating foreign alphabet characters, the tokens were padded at the start and at the end of each sequence. A single flattened list was then created with each letter from the corpus forming a separate item.  \n",
        "\n",
        "Using the nltk.lm.Vocabulary class, a list containing every letter present more than 50 times throughout the corpus was made. Analysis of the final dataset revealed a total length of 5737483 letters; a vocabulary set of 31 letters, which, upon inspection, included all letters of the English alphabet, as well as both designed chosen padding symbols (“< s>” and “< /s>”); while the total length of unique letters in the text was found to be 43, containing letters whose frequency was deemed to low to be relevant for the task ('ö', 'á', 'í', 'â', 'é', 'œ', 'à', 'ê', 'ù', 'ï', 'ë', 'ô', 'è'). Finally, the list was separated into a training set, which comprised all the letters in the corpus apart of the first 10000, which were selected to be used for the test set.\n",
        "\n",
        "The aim of the neural network will be to predict which letter from the vocabulary is more likely to follow a text sequence - to this end, an index was then created to obtain a numerical equivalent of each letter in the list based on their position in the vocabulary. After indexing the train and test sets, the data was reshaped into an array by partitioning each set into arrays of shape (, 80) to obtain sequences of similar length: the training set, of dimension (22549, 80), and the test set, of dimension (400, 80). An 80/20 split between the training and validation test was chosen for training over 200 epochs, with EarlyStopping being set at 10 to avoid overfitting - the model will monitor the loss function for the validation set and stop if the gradient descent has not decreased for more than 10 epochs.\n",
        "\n",
        "2.\tRecurrent neural network model\n",
        "\n",
        "A sequential recurrent neural network model was created with the following characteristics:\n",
        "\n",
        "a.\tEmbedding layer\n",
        "\n",
        "The Embedding layer transforms each one-hot encoded input into a unique vector, which then is used by the language model to keep track of the relationships between the tokens; The letters that appear in similar context will have similar embeddings. The high-dimensional data is compressed, and the processing of dense vectors can make the model faster and more efficient.\n",
        "The present model has been trained with vectors of different lengths - an input dimension of 28 yielded relatively high perplexity (5.87) and loss (1.76) values, while higher vector dimensions of length 64 (Perplexity = , Loss = ) and 128 (Perplexity = , Loss = ) showed significantly lower results. This may be due to the complexity of the dataset, a longer vector may be able to capture more complex relationships between letters in the corpus\n",
        "\n",
        "b.\t Long Short-Term Model\n",
        "\n",
        "After coverting our tokens to meaningful representations, an LSTM layer with 32 memory units was used to predict the next letter in the sequence while keeping track of the previous input.The network's internal gates allow the model to be trained through backpropagation and prevent vanishing gradient. The model returns the hidden state output for each input time step.\n",
        "\n",
        "c. Dropout layer\n",
        "The LSTM was followed by a dropout layer; this layer will randomly set 20% of the weights to zero before feeding the input to a second LSTM layer, which will have to re-learn some of the lost input. This layer has been added to avoid overfitting of the data.\n",
        "\n",
        "d.\tLong Short Term Model with dropout\n",
        "The second LSTM model, of shape 32, updates the weights based on the input received by the dropout layer, re-learns some of the lost relationships between the tokens and forwards it to the next layer\n",
        "\n",
        "d.\tDense layer\n",
        "A final dense layer takes the output from the LSTM model and, using a softmax function, forces it into an integer between 0 and 1 representing. Opposite to the embedding layer, its role is to take the embeddings and return them to their one-hot encoded form\n",
        "\n",
        "After ... epochs, the training was stopped as the loss function for the validation set stopped decreasing. Two graphs were made showing the Loss and Perplexity values for both the training and validation sets. Inspection of the graphs shows that both sets are descending into convergence; however, the model does not present overfitting.\n",
        "By using the model.predict() function, it's possible to confirm that the model generalises relatively well to unseen data, with the loss and perplexity values being similar to the ones observed at the local minimum for this model (Perplexity = , Loss = ). This result shows that the model's predictions of the next letter are as good as a random guess between six of the vocabulary characters. This remains a high\n",
        "\n",
        "The embedding layer\n",
        "7.2 work heavily with nltk we use several nltk classes such as vocabulary and utility functions such as pad_sequences neural networks are taken from tensorflow, adam the optimizer, time distributed is a layer that connects structure from a feedforward network to the lstm\n",
        "padding to be able to distinguish between sentences create a vocabulary giving cutoff as a parameters\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UGmaej6DpKxZ-YXKPbCP7CfKyFrCgDea",
      "authorship_tag": "ABX9TyODTo6YYZa6vldeV3DHzEYu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}