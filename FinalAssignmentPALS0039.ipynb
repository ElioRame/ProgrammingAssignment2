{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/FinalAssignmentPALS0039.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Mn5DPIaA8HGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b730fba3-b555-4524-f6ea-801f759a87b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe Project Gutenberg eBook of The Adventures of Tom Sawyer, Complete\\n', '    \\n', 'This ebook is for the use of anyone anywhere in the United States and\\n', 'most other parts of the world at no cost and with almost no restrictions\\n', 'whatsoever. You may copy it, give it away or re-use it under the terms\\n', 'of the Project Gutenberg License included with this ebook or online\\n', 'at www.gutenberg.org. If you are not located in the United States,\\n', 'you will have to check the laws of the country where you are located\\n', 'before using this eBook.\\n', '\\n', 'Title: The Adventures of Tom Sawyer, Complete\\n', '\\n', 'Author: Mark Twain\\n', '\\n', 'Release date: July 1, 2004 [eBook #74]\\n', '                Most recently updated: August 9, 2023\\n', '\\n', 'Language: English\\n', '\\n', 'Credits: David Widger\\n', '\\n', '\\n', '*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF TOM SAWYER, COMPLETE ***\\n', '\\n', '\\n', '\\n', '\\n', 'THE ADVENTURES OF TOM SAWYER\\n', '\\n', '\\n', 'By Mark Twain\\n', '\\n', '(Samuel Langhorne Clemens)\\n', '\\n', '\\n', '\\n', '\\n', 'CONTENTS\\n', '\\n', '\\n', 'CHAPTER I. Y-o-u-u Tom—Aunt Polly Decides Upon her Duty—Tom Practices\\n', 'Music—The Challenge—A Private Entrance\\n', '\\n', 'CHAPTER II. Strong Temptations—Strategic Movements—The Innocents\\n', 'Beguiled\\n', '\\n', 'CHAPTER III. Tom as a General—Triumph and Reward—Dismal\\n', 'Felicity—Commission and Omission\\n', '\\n', 'CHAPTER IV. Mental Acrobatics—Attending Sunday—School—The\\n']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(\"/content/pg74.txt\") as TomSawyer:\n",
        "  Book1 = TomSawyer.readlines()\n",
        "with open(\"/content/pg84.txt\") as Frankenstein:\n",
        "  Book2 = Frankenstein.readlines()\n",
        "with open(\"/content/pg345.txt\") as Dracula:\n",
        "  Book3 = Dracula.readlines()\n",
        "with open(\"/content/pg1342.txt\") as PrideandPrejudice:\n",
        "  Book4 = PrideandPrejudice.readlines()\n",
        "with open(\"/content/pg1727.txt\") as Odissey:\n",
        "  Book5 = Odissey.readlines()\n",
        "with open(\"/content/pg2701.txt\") as MobyDick:\n",
        "  Book6 = MobyDick.readlines()\n",
        "with open(\"/content/pg3207.txt\") as Leviathan:\n",
        "  Book7 = Leviathan.readlines()\n",
        "\n",
        "data = Book1 + Book2 + Book3 + Book4 + Book6 + Book7\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-T2aK-DX8UO",
        "outputId": "7501ae9d-465c-4fae-8e16-11c6e45f621c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install -U nltk>=3.7.0\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import pad_sequence, ngrams\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.models import Laplace\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, LSTM, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q1vaRZJYI2f",
        "outputId": "3d0012b9-7658-4dd4-d0b5-1d619c24fa72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of corpus: 3845425\n",
            "Length of vocabulary: 42\n",
            "Unique words in corpus: 59\n",
            "['t', 'h', 'e', 'p', 'r', 'o', 'j', 'c', 'g', 'u', 'n', 'b', 'k', 'f', 'a', 'd', 'v', 's', 'm', 'w', 'y', 'l', 'i', '1', '2', '0', '4', '7', '9', '3', 'z', 'x', 'q', '8', '6', '_', 'é', 'ê', '5', 'æ', 'è', '<UNK>']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import collections\n",
        "import operator\n",
        "\n",
        "\n",
        "letters = []\n",
        "padded_letters = []\n",
        "\n",
        "for text in data:\n",
        "  lower_text = [word.lower() for word in text]\n",
        "  for words in lower_text:\n",
        "   tokenizer = RegexpTokenizer(r'\\w+')\n",
        "   tokenised_word = tokenizer.tokenize(words)\n",
        "   for word in tokenised_word:\n",
        "    for letter in word:\n",
        "      letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "   #for punct in string.punctuation:\n",
        "    #letters = list(map(lambda x: \" \" if x == punct else x, tokenised_word))\n",
        "     #for punct in string.punctuation:\n",
        "      #text_punct = list(map(lambda x: \" \" if x == punct else x, letter))\n",
        "      #letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "#def tokenize(text):\n",
        "  #text_lower = [word.lower() for word in str(text)]\n",
        "  #for punct in string.punctuation:\n",
        "    #text_punct = list(map(lambda x: \" \" if x == punct else x, text_lower))\n",
        "  #return text_punct\n",
        "\n",
        "#tokenized_data = tokenize(data)\n",
        "#for words in tokenized_data:\n",
        "  #for letter in words:\n",
        "    #letters.append(letter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "padded_lower_s = list(pad_sequence(letters,\n",
        "                                     n=2,\n",
        "                                     pad_left=True,\n",
        "                                     left_pad_symbol=\"<s>\",\n",
        "                                     pad_right=True,\n",
        "                                     right_pad_symbol=\"</s>\"))\n",
        "padded_letters.append(padded_lower_s)\n",
        "\n",
        "\n",
        "# 2. Flatten and create finite vocabulary\n",
        "words = list(flatten(padded_letters))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocab = Vocabulary(words, unk_cutoff=10)\n",
        "\n",
        "# 3. Partition into train and test\n",
        "test_words = words[:10000]\n",
        "train_words = words[10000:]\n",
        "\n",
        "#Inspect\n",
        "print(\"Length of corpus:\", len(words))\n",
        "print(\"Length of vocabulary:\", len(vocab))\n",
        "print(\"Unique words in corpus:\", len(set(words)))\n",
        "\n",
        "v = [word for word in vocab]\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N55ekRQ7Ye72",
        "outputId": "490d7b0d-3b1b-4090-9642-a395bde82f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22, 17, 11, 2, 0, 0, 2, 4, 10, 20, 5, 9, 21, 5, 5, 12, 35, 0, 1, 22, 17, 35, 0, 22, 18]\n",
            "[41, 0, 1, 2, 3, 4, 5, 6, 2, 7, 0, 8, 9, 0, 2, 10, 11, 2, 4, 8, 2, 11, 5, 5, 12]\n"
          ]
        }
      ],
      "source": [
        "#Create mapping\n",
        "word_to_index = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "#Map data\n",
        "train_indices = [word_to_index[vocab.lookup(w)] for w in train_words]\n",
        "test_indices = [word_to_index[vocab.lookup(w)] for w in test_words]\n",
        "\n",
        "#Inspect\n",
        "print(train_indices[:25])\n",
        "print(test_indices[:25])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45gOwWriYuc-",
        "outputId": "9e86c4b3-a4d0-431b-bd0a-0c55792ea471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38354, 100) (38354, 100)\n",
            "(100, 100) (100, 100)\n"
          ]
        }
      ],
      "source": [
        "seqlen = 100\n",
        "\n",
        "def prepare_sequences(words, unk_index, seqlen=seqlen):\n",
        "  trunc_length = len(words) % seqlen\n",
        "  X = np.array(words)[trunc_length:].reshape((-1, seqlen))\n",
        "  y = np.array(words + [unk_index])[trunc_length + 1:].reshape((-1, seqlen))\n",
        "  return X, y\n",
        "\n",
        "Xtrain, ytrain = prepare_sequences(train_indices, word_to_index[\"<UNK>\"])\n",
        "Xtest, ytest = prepare_sequences(test_indices, word_to_index[\"<UNK>\"])\n",
        "\n",
        "#Inspect\n",
        "print(Xtrain.shape, ytrain.shape)\n",
        "print(Xtest.shape, ytest.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "DMauwpOHY2Ff",
        "outputId": "63e08bd0-4a6c-43d0-b0e0-8d2b27ecc09c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_8              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_8              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def perplexity(y_true, y_pred):\n",
        "  cross_entropy = tf.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "  return perplexity\n",
        "\n",
        "def build_model(input_width=len(vocab), input_length=seqlen, output_width=len(vocab), learning_rate=0.01):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=input_width, output_dim=64, input_length=input_length))\n",
        "  model.add(LSTM(32, return_sequences=True, activation='tanh'))\n",
        "  model.add(Dense(32, activation='tanh'))\n",
        "  model.add(TimeDistributed(Dense(output_width, activation='softmax')));\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=[perplexity])\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "#Inspect\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfrg_QKHY6lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3da987-69be-430c-ac2a-a100bd247fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m540/540\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 131ms/step - loss: 2.0249 - perplexity: 7.5772 - val_loss: 1.9859 - val_perplexity: 7.2909\n",
            "Epoch 2/40\n",
            "\u001b[1m258/540\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 121ms/step - loss: 2.0291 - perplexity: 7.6093"
          ]
        }
      ],
      "source": [
        "training_info = model.fit(Xtrain,ytrain, batch_size=64, validation_split=0.10, epochs=40, callbacks=EarlyStopping(monitor=\"val_loss\", patience=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_YSlhYIbQSQ"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(training_info):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "  axs[0].plot(training_info.history['loss'], label=\"training set\")\n",
        "  axs[0].plot(training_info.history['val_loss'], label=\"validation set\")\n",
        "  axs[0].set_xlabel(\"Epoch\")\n",
        "  axs[0].set_ylabel(\"Loss\")\n",
        "  axs[0].grid(True)\n",
        "  axs[0].legend()\n",
        "  try:\n",
        "    axs[1].plot(training_info.history['perplexity'], label=\"training set\")\n",
        "    axs[1].plot(training_info.history['val_perplexity'], label=\"validation set\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].set_ylabel(\"Perplexity\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].legend()\n",
        "  except:\n",
        "    pass\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(training_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzpCeOiBbVDo"
      },
      "outputs": [],
      "source": [
        "test_loss, test_perplexity = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt6uPTLebal5"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import rv_discrete\n",
        "\n",
        "index_to_word = {i: w for w, i in word_to_index.items()}\n",
        "\n",
        "# Choose seed text and create padded index sequence\n",
        "seed_text = \"it is a fact that\"\n",
        "for w in seed_text.split():\n",
        "  tokenizer.tokenize(w)\n",
        "  for word in w:\n",
        "    for letters in word:\n",
        "     seq = [word_to_index[l] for l in letters]\n",
        "padded_seq = pad_sequences([seq], maxlen=seqlen, padding='post')\n",
        "\n",
        "# Sample word sequence\n",
        "for i in range(seqlen - len(seq)):\n",
        "  full_prediction = model.predict(padded_seq, verbose=0)\n",
        "  next_one_letter = full_prediction[0][len(seq) + i, :]\n",
        "  next_two_letters = next_one_letter[0][len(seq) + i, :]\n",
        "  next_three_letters = next_two_letters[0][len(seq) + i, :]\n",
        "  next_three_letters_prediction = [next_one_letter, next_two_letters, next_three_letters]\n",
        "  dpd = rv_discrete(values=(list(range(len(next_three_letters_prediction))), next_three_letters_prediction))\n",
        "  padded_seq[0][len(seq) + i] = dpd.rvs(size=1)\n",
        "\n",
        "# Print word sequence\n",
        "sampled_words = []\n",
        "for word_index in padded_seq[0]:\n",
        "  sampled_words.append(index_to_word[word_index])\n",
        "print(\" \".join(sampled_words))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UGmaej6DpKxZ-YXKPbCP7CfKyFrCgDea",
      "authorship_tag": "ABX9TyPmd3k5q6KUNAQ4fibi/gWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}