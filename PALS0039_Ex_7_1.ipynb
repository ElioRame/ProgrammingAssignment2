{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/PALS0039_Ex_7_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QYlAUr-y-kX"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "# Exercise 7.1 N-gram language modelling using NLTK\n",
        "\n",
        "In this exercise we experiment with n-gram language models using [`NLTK`'s functionality](https://www.nltk.org/api/nltk.lm.html).\n",
        "\n",
        "You might also find the following article insightful: [Language Modeling with NLTK](https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znfr9EdrPcoV",
        "outputId": "ab6dcaa9-4534-450d-98bc-0fb655e30574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U nltk>=3.7.0\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "from nltk.lm import Vocabulary\n",
        "from nltk.util import pad_sequence, ngrams\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.models import Laplace"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) ***Collect*** all the sentences from the `reuters` corpus, ***lowercase*** them, and ***pad*** the start and end with special symbols (this means we will have n-grams that distinguish the start and end of sentences). For the left pad symbol use `<s>` and the right use `</s>`.\n",
        "\n",
        "Hint: Use the [`reuters.sents()`](https://www.nltk.org/api/nltk.corpus.html#corpus-reader-functions) method and the `pad_sequence` function that have already been imported"
      ],
      "metadata": {
        "id": "YEv1-ttOkOLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(a)\n",
        "sentences = []\n",
        "for s in reuters.sents():\n",
        "  lower_s = [word.lower() for word in s]\n",
        "  padded_lower_s = list(pad_sequence(lower_s,\n",
        "                                     n=2,\n",
        "                                     pad_left=True,\n",
        "                                     left_pad_symbol=\"<s>\",\n",
        "                                     pad_right=True,\n",
        "                                     right_pad_symbol=\"</s>\"))\n",
        "  sentences.append(padded_lower_s)\n",
        "\n",
        "#Inspect the first 3 sentences\n",
        "for s in sentences[:3]:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "CSvb1BYkjXJl",
        "outputId": "b3ceed70-ea18-4f4f-8dcd-a67210abe166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', '</s>']\n",
            "['<s>', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u', '.', 's', '.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u', '.', 's', '.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.', '</s>']\n",
            "['<s>', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', '-', 'run', ',', 'in', 'the', 'short', '-', 'term', 'tokyo', \"'\", 's', 'loss', 'might', 'be', 'their', 'gain', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) If needed, ***flatten*** the sentences into a single list of words representing the entire corpus and create a finite vocabulary using NLTK's [`Vocabulary` constructor](https://www.nltk.org/api/nltk.lm.html#nltk.lm.Vocabulary) by specifying a frequency cut-off at 10.\n",
        "\n",
        "(c) Subsequently, inspect the lengths of the corpus and the vocabulary. Compare the length of the vocabulary with the number of unique words in the corpus."
      ],
      "metadata": {
        "id": "AgzFpOs6pFWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(b)\n",
        "# ANSWER\n",
        "counts = []\n",
        "for sentence in sentences:\n",
        "  for i in sentence:\n",
        "    counts.append(i)\n",
        "\n",
        "vocab = Vocabulary(counts, unk_cutoff = 10)\n",
        "\n",
        "#(c)\n",
        "# ANSWER\n",
        "print(\"Counts length is:\", len(counts), \", while Vocab length is:\", len(vocab))\n"
      ],
      "metadata": {
        "id": "hwOuOGE3hxGC",
        "outputId": "0511f4bf-76be-45fa-b57d-2684b994c1e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts length is: 1830349 , while Vocab length is: 8070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Split the text into `train` and `test` sets as follows: reserve the first 10,000 words for the `test` set and the rest for `train`."
      ],
      "metadata": {
        "id": "KHmjksbV6WXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(d)\n",
        "# ANSWER\n",
        "test_words = counts[:10000]\n",
        "train_words = counts[10000:]"
      ],
      "metadata": {
        "id": "80iggfyP6mAV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train three n-gram language models with n=1,2,3 respectively. Use add-one smoothing (using NLTK's [`Laplace` constructor]())"
      ],
      "metadata": {
        "id": "tEV19XNn6w6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lms = {}\n",
        "\n",
        "for n in [1, 2, 3]:\n",
        "  train_ngrams = list(ngrams(train_words, n))\n",
        "  print(train_ngrams[:10])\n",
        "  lm = Laplace(n)\n",
        "  lm.fit([train_ngrams], vocab)\n",
        "  lms[n] = lm"
      ],
      "metadata": {
        "id": "iguvnMrX8kg8",
        "outputId": "92ad6239-8c1c-4a7f-d173-3b3eb16ef6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reasons',), ('are',), ('low',), ('domestic',), ('inflation',), (',',), ('a',), ('bottoming',), ('out',), ('of',)]\n",
            "[('reasons', 'are'), ('are', 'low'), ('low', 'domestic'), ('domestic', 'inflation'), ('inflation', ','), (',', 'a'), ('a', 'bottoming'), ('bottoming', 'out'), ('out', 'of'), ('of', 'the')]\n",
            "[('reasons', 'are', 'low'), ('are', 'low', 'domestic'), ('low', 'domestic', 'inflation'), ('domestic', 'inflation', ','), ('inflation', ',', 'a'), (',', 'a', 'bottoming'), ('a', 'bottoming', 'out'), ('bottoming', 'out', 'of'), ('out', 'of', 'the'), ('of', 'the', 'fall')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Evaluate each of the 3 language models by determining the perplexity on the `train` and `test` sets.\n",
        "\n",
        "Hint: Use the [`perplexity` method](https://www.nltk.org/api/nltk.lm.api.html#nltk.lm.api.LanguageModel.perplexity)"
      ],
      "metadata": {
        "id": "D_RbK2MB9lcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(e)\n",
        "# ANSWER\n",
        "for n in [1, 2, 3]:\n",
        "  train_ngrams = list(ngrams(train_words, n))\n",
        "  test_ngrams = list(ngrams(test_words, n))\n",
        "\n",
        "  print(\"--------------------------------------------------\")\n",
        "  print(f\"Results for {n}-gram model:\")\n",
        "  print(\"Train perplexity:\", lms[n].perplexity(train_ngrams))\n",
        "  print(\"Test perplexity :\", lms[n].perplexity(test_ngrams))\n"
      ],
      "metadata": {
        "id": "MWG6ePz79l0x",
        "outputId": "79ec8e77-33dc-426d-dd83-97dc7f01168c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Results for 1-gram model:\n",
            "Train perplexity: 539.4316902192742\n",
            "Test perplexity : 557.7335208906071\n",
            "--------------------------------------------------\n",
            "Results for 2-gram model:\n",
            "Train perplexity: 219.0124849104525\n",
            "Test perplexity : 293.8424181331866\n",
            "--------------------------------------------------\n",
            "Results for 3-gram model:\n",
            "Train perplexity: 735.2921560827871\n",
            "Test perplexity : 1236.9728803996954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Comment on the performance of the three models, which one is best. Why?"
      ],
      "metadata": {
        "id": "8fkApeNVAar3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "# ANSWER\n",
        "#2-gram\n",
        "#3-gram starts to exhibit overfitting"
      ],
      "metadata": {
        "id": "V2MWn7tYAkVZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) What would the perplexity be for a predictor which randomly guesses from any one of the words occurring in the test set?"
      ],
      "metadata": {
        "id": "VXmPI1orBpf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(g)\n",
        "# ANSWER\n",
        "len(set(test_words))"
      ],
      "metadata": {
        "id": "Q_rJeobSBzqn",
        "outputId": "8ade3e3d-e363-4926-f6d6-694d279a5b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2075"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(h) **Optional:** Experiment with models using different smoothing approaches. What is the best perplexity you can achieve?"
      ],
      "metadata": {
        "id": "yrMeS72JCLgQ"
      }
    }
  ]
}