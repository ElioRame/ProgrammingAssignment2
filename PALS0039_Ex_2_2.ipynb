{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/PALS0039_Ex_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCvRvja7Yv9m"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 2.2 Regression task\n",
        "\n",
        "In this exercise we set up a simple regression task and explore how well different regression models fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olG5JLfUY26F"
      },
      "source": [
        "(a) The following code generates some random training and test data for a regression problem. Run the code, then add comments to explain the different steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfaQZe_7Yu6x"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#\n",
        "np.random.seed(0)\n",
        "#a random seed is set to make sure that random numbers are replicable\n",
        "#\n",
        "def linear_polynomial(x, gradient=4.0, intercept=2.0):\n",
        "  return gradient * x + intercept\n",
        "#gradient = slope, intercept = where the line intercepts the y\n",
        "#\n",
        "def generate_noisy_samples(num_samples, domain=[0.0, 1.0]):\n",
        "  x = np.linspace(domain[0], domain[1], num_samples)\n",
        "  y = linear_polynomial(x) + np.random.normal(size=num_samples)\n",
        "  return x, y\n",
        "#x = np.linspace creates a specified number (num_samples) evenly spaced numbers over an interval (from 0 to 1)\n",
        "#the evenly spaced numbers are fed into the linear-polynomial functin which creates a gradually ascending linear polyniomial by multiplying lower to higher even spaced numbers to same gradient and intercept\n",
        "#randomness is added with np.random.normal numbers of same size as num_samples\n",
        "x_train, y_train = generate_noisy_samples(100)\n",
        "x_test, y_test = generate_noisy_samples(100)\n",
        "#random samples are assigned to train and test sets\n",
        "#when plotting the x_train and y-train are noisy, but by reapplying the linear_polynomial functions to the carefully crafted noisy samples, the relationship becomes apparent\n",
        "#\n",
        "plt.plot(x_train, y_train,'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train), label=\"true relationship\", linestyle=\"--\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Use the numpy [`Polynomial.fit` method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html) to find a polynomial with **degree of 0** from the training data. Plot this function against the training data (as above).\n",
        "\n",
        "Hint: to evaluate the estimated model returned by `Polynomial.fit` you can simply use it as a callable function that takes x values as input."
      ],
      "metadata": {
        "id": "YFphmm9HOmKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.polynomial import Polynomial\n",
        "poly0 = Polynomial.fit(x_train, y_train, deg = 0)\n",
        "#(b)\n",
        "#ANSWER\n",
        "plt.plot(x_train, y_train,'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train), label=\"true relationship\", linestyle=\"--\")\n",
        "plt.plot(x_train, poly0(x_train))\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7C7g7iL8PTa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Repeat the previous task but fit two additional models with **degree 1 and 2** respectively."
      ],
      "metadata": {
        "id": "_Nx3dM1yP5w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c)\n",
        "#ANSWER\n",
        "poly1 = Polynomial.fit(x_train, y_train, deg=1)\n",
        "poly2 = Polynomial.fit(x_train, y_train, deg=2)\n",
        "\n",
        "plt.plot(x_train, y_train,'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train), label=\"true relationship\", linestyle=\"--\")\n",
        "plt.plot(x_train, poly0(x_train))\n",
        "plt.plot(x_train, poly1(x_train))\n",
        "plt.plot(x_train, poly2(x_train))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QeYpKkcOQIqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqJhR7pyZUiU"
      },
      "source": [
        "(d) Calculate the **mean squared error** (MSE) on both the training and test sets for the three polynomial models.\n",
        "\n",
        "Hint: SKLearn has a function that can be used to [calculate the MSE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ9SAPXbZixM"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "y_train_predicted_0 = poly0(x_train)\n",
        "y_test_predicted_0  = poly0(x_test)\n",
        "y_train_predicted_1 = poly1(x_train)\n",
        "y_test_predicted_1  = poly1(x_test)\n",
        "y_train_predicted_2 = poly2(x_train)\n",
        "y_test_predicted_2  = poly2(x_test)\n",
        "#fitting a polynomial of deg 0, 1, and 2 to the two random train and test sets. it is expected that deg 1 will approximate the true relationship better for both tests as deg 0 is too general and deg 2 is too fitted to the x_train data\n",
        "#considering that the x_test data, while still linear, contains randomness (as do all the sets here), the exercise works because one of the models will be necessarily underfitted and on overfitted\n",
        "mses_train = {}\n",
        "mses_test = {}\n",
        "#the lower the MSE, the better\n",
        "mses_train[\"poly0\"] = mean_squared_error(y_train_predicted_0, y_train)\n",
        "mses_train[\"poly1\"] = mean_squared_error(y_train_predicted_1, y_train)\n",
        "mses_train[\"poly2\"] = mean_squared_error(y_train_predicted_2, y_train)\n",
        "\n",
        "mses_test[\"poly0\"] = mean_squared_error(y_test_predicted_0, y_test)\n",
        "mses_test[\"poly1\"] = mean_squared_error(y_test_predicted_1, y_test)\n",
        "mses_test[\"poly2\"] = mean_squared_error(y_test_predicted_2, y_test)\n",
        "#comparing the new polynomial to the noisy initial data\n",
        "print(\"TRAIN:\", mses_train, sep=\"\\t\")\n",
        "print(\"TEST:\", mses_test, sep=\"\\t\")\n",
        "#(d)\n",
        "#ANSWER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Can you identify underfitting and overfitting? Which model would you eventually deploy on this task? Why?"
      ],
      "metadata": {
        "id": "MmqEUoJOY2jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(e)\n",
        "#ANSWER\n",
        "#poly0 = underfitting: poor performance on both train and test\n",
        "#poly1 = good fit: performance better for test but not too much, it hasn't been trained so much that it overapproximates a specific dataset and can generalise results\n",
        "#poly2 = overfitting: better performance for train test, poor generalisation"
      ],
      "metadata": {
        "id": "arU008pLZHhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Pick a regression method of your choice from the `sklearn` library (regression models with example code can be found in [this index](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)). Identify some of the hyperparameters of this model and try to find out how they affect the modelling."
      ],
      "metadata": {
        "id": "QQ_G4mdIZTCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "#ANSWER\n",
        "#DecisionTreeRegressor: max_depth, min_samples_split, min_samples_leaf, ..."
      ],
      "metadata": {
        "id": "Z52DnGTIaKWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) Fit your choice of regression model with two or three different configurations of the hyperparamers and calculate the train and test set MSE for this model (as done before). Which model would you eventually deploy on this task?"
      ],
      "metadata": {
        "id": "dyxZdSBPaUCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(g)\n",
        "#ANSWER\n",
        "#from professor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_d2 = DecisionTreeRegressor(max_depth=2).fit(x_train.reshape(-1, 1), y_train)\n",
        "tree_d4 = DecisionTreeRegressor(max_depth=4).fit(x_train.reshape(-1, 1), y_train)\n",
        "tree_d8 = DecisionTreeRegressor(max_depth=8).fit(x_train.reshape(-1, 1), y_train)\n",
        "#a decision tree approximates a sine curve over the data to then predict the value of a target variable by implementng supervised if-else decisions.\n",
        "#the depth refers to the greater or lesser attention to fine detailes and features of the data when making the prediction, here we can see that the higher depths exibit overfitting\n",
        "#decision trees require X to have two features (n_samples, n_features), it is possible to reshape the values as done above - the array was changed to be of 2 rather than 1 dimension - while not adding anything for the n_features option (achieved through the .reshape(1, -1)\n",
        "#this makes sense here but making the decision tree ignore the number of features could make it less acurate (?) because it can make less \"splits\"\n",
        "\n",
        "mses_train = {}\n",
        "mses_test = {}\n",
        "\n",
        "mses_train[\"tree_d2\"] = mean_squared_error(tree_d2.predict(x_train.reshape(-1, 1)), y_train)\n",
        "mses_train[\"tree_d4\"] = mean_squared_error(tree_d4.predict(x_train.reshape(-1, 1)), y_train)\n",
        "mses_train[\"tree_d8\"] = mean_squared_error(tree_d8.predict(x_train.reshape(-1, 1)), y_train)\n",
        "\n",
        "mses_test[\"tree_d2\"] = mean_squared_error(tree_d2.predict(x_test.reshape(-1, 1)), y_test)\n",
        "mses_test[\"tree_d4\"] = mean_squared_error(tree_d4.predict(x_test.reshape(-1, 1)), y_test)\n",
        "mses_test[\"tree_d8\"] = mean_squared_error(tree_d8.predict(x_test.reshape(-1, 1)), y_test)\n",
        "\n",
        "print(\"TRAIN:\", mses_train, sep=\"\\t\")\n",
        "print(\"TEST:\", mses_test, sep=\"\\t\")\n",
        "\n",
        "plt.plot(x_train, y_train,'bo', label=\"noisy training samples\")\n",
        "plt.plot(x_train, linear_polynomial(x_train), label=\"true relationship\", linestyle=\"--\")\n",
        "plt.plot(x_train, tree_d2.predict(x_train.reshape(-1, 1)), label=\"tree with max depth 2\")\n",
        "plt.plot(x_train, tree_d4.predict(x_train.reshape(-1, 1)), label=\"tree with max depth 4\")\n",
        "plt.plot(x_train, tree_d8.predict(x_train.reshape(-1, 1)), label=\"tree with max depth 8\")\n",
        "plt.show()\n",
        "\n",
        "y_train.shape\n",
        "x_train.shape\n",
        "x_for_tree = x_train.reshape(-1, 1)\n",
        "x_train.ndim\n",
        "#Deploy tree_d2 (best generalisation error) -- other models exhibited overfitting!"
      ],
      "metadata": {
        "id": "zuW-LI3marDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}