{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElioRame/ProgrammingAssignment2/blob/master/PALS0039_Ex_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVyGSWa50Jli"
      },
      "source": [
        "[![PALS0039 Logo](https://www.phon.ucl.ac.uk/courses/pals0039/images/pals0039logo.png)](https://www.phon.ucl.ac.uk/courses/pals0039/)\n",
        "\n",
        "#Exercise 2.3 Classification task\n",
        "\n",
        "In this exercise we train a model to classify vowels from their [formant frequencies](https://en.wikipedia.org/wiki/Formant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxVFZU60ONa"
      },
      "source": [
        "The following code reads in and summarises a data set of vowel formant frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etXkZPRl0GTj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df = pd.read_csv(\"https://www.phon.ucl.ac.uk/courses/pals0039/data/exercise_02/vowels.csv\")\n",
        "\n",
        "print(df)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Create three boxplots that show the differences in F1, F2, and HEIGHT between male and female samples.\n",
        "\n",
        "Hint: You could use [`plt.subplots`](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html) to position all the plots in a row or column. You could use the [`boxplot` method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html) of `DataFrame` to create each plot."
      ],
      "metadata": {
        "id": "POi8JLOK1R2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(a)\n",
        "#ANSWER\n",
        "fig, axs = plt.subplots(2,3, figsize = (7, 12))\n",
        "\n",
        "female = df.loc[df[\"SEX\"] == \"female\"]\n",
        "print(female)\n",
        "F1_female = female[\"F1\"]\n",
        "F2_female = female[\"F2\"]\n",
        "Height_female = female[\"HEIGHT\"]\n",
        "\n",
        "male = df.loc[df[\"SEX\"] == \"male\"]\n",
        "F1_male = male[\"F1\"]\n",
        "F2_male = male[\"F2\"]\n",
        "Height_male = male[\"HEIGHT\"]\n",
        "\n",
        "axs[0, 0].boxplot(F1_female)\n",
        "axs[0, 0].set_title('F1 Female')\n",
        "\n",
        "\n",
        "axs[0, 1].boxplot(F2_female)\n",
        "axs[0, 1].set_title('F2 Female')\n",
        "\n",
        "\n",
        "axs[0, 2].boxplot(Height_female)\n",
        "axs[0, 2].set_title('Female Height')\n",
        "\n",
        "\n",
        "axs[1, 0].boxplot(F1_male)\n",
        "axs[1, 0].set_title('F1 Male')\n",
        "\n",
        "\n",
        "axs[1, 1].boxplot(F2_male)\n",
        "axs[1, 1].set_title('F2 Male')\n",
        "\n",
        "\n",
        "axs[1, 2].boxplot(Height_male)\n",
        "axs[1, 2].set_title('Male Height')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ua-BxNSx2QBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntNyatxm5Szp"
      },
      "source": [
        "---\n",
        "(b) This code plots an F1-F2 scatter plot in which different vowels are displayed in different colours. Run the code and then add comments to the code to describe what is happening in each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2N3kjul5-R1"
      },
      "source": [
        "#(b)\n",
        "#\n",
        "df[\"VOWEL\"]=df.VOWEL.astype(\"category\")\n",
        "#extracts categories\n",
        "#\n",
        "df[\"VOWELIDX\"]=df.VOWEL.cat.codes\n",
        "#creates a new column with category codes to then feed to the colour argument in plt.scatter\n",
        "#\n",
        "def plot_formants(data, f1=\"F1\", f2=\"F2\", axis_ranges=[3000,500,1100,100]):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.scatter(data[f2], data[f1], c=data.VOWELIDX, cmap=\"tab10\")\n",
        "  if axis_ranges: plt.axis(axis_ranges)\n",
        "  plt.xlabel(\"F2\")\n",
        "  plt.ylabel(\"F1\")\n",
        "  plt.grid()\n",
        "\n",
        "plot_formants(df)\n",
        "plt.show()\n",
        "print(df[\"VOWELIDX\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Interpreting the above scatter plot: Will perfect classification be possible using these measurements (F1 and F2 in Hz)? Why?"
      ],
      "metadata": {
        "id": "BESvXCcJ_LFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c)\n",
        "#ANSWER\n",
        "#No, becayse the different vowels superimpose too much and cannot really be separated by f1 and f2 alone"
      ],
      "metadata": {
        "id": "IermhiJV_j4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below randomly selects a small held-out test set (which is plotted). The remaining samples are defined as the training set.\n",
        "\n",
        "(d) Is the test set fully representative of the task? Why?"
      ],
      "metadata": {
        "id": "i8pytWYlBRPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = df.sample(frac=0.05, random_state=0)\n",
        "print(test_set.describe())\n",
        "#extracting 5% of data\n",
        "train_set = df.drop(test_set.index)\n",
        "print(train_set.describe())\n",
        "\n",
        "plot_formants(test_set)\n",
        "plt.show()\n",
        "\n",
        "print(train_set, test_set)\n",
        "#(d)\n",
        "#ANSWER\n",
        "#No, vowels are missing and the set is too sparse"
      ],
      "metadata": {
        "id": "aXec9a41DXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Use `sklearn` to train a [Nearest Neighbour Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on the training set. The inputs should be `F1` and `F2` and the output should be the `VOWEL`. Configure the classifier to use the 3 nearest neighbours.\n",
        "\n",
        "Hint: You need to define the classifier then call the [`fit` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit)"
      ],
      "metadata": {
        "id": "ol28m6XAJlHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#(e)\n",
        "#ANSWER\n",
        "vowel = KNeighborsClassifier(n_neighbors = 3)\n",
        "vowel.fit(X= train_set[['F1','F2']], y = train_set[\"VOWEL\"])"
      ],
      "metadata": {
        "id": "qCKcjoeNKMrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Determine the classification accuracy of your classifier on the train and test sets.\n",
        "\n",
        "Hint: You can use the [`score` method](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) of the classifier."
      ],
      "metadata": {
        "id": "kj-adFEoLeWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P1xjSXKvqyUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(f)\n",
        "#ANSWER\n",
        "print(\"The train set accuracy is:\", vowel.score(X = train_set[[\"F1\", \"F2\"]], y = train_set[[\"VOWEL\"]]), sep = \"\\t\")\n",
        "print(\"The test set accuracy is:\", vowel.score(X = test_set[[\"F1\", \"F2\"]], y = test_set[[\"VOWEL\"]]), sep = \"\\t\")"
      ],
      "metadata": {
        "id": "CCS4g-4LLpXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(g) The following code normalises the data using the [z-score](https://en.wikipedia.org/wiki/Standard_score) for each speaker individually.\n",
        "\n",
        "Add comments to each code block to explain what is happening."
      ],
      "metadata": {
        "id": "dy70BjK3OmeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(g)\n",
        "#\n",
        "#means = df.groupby(['SPEAKER']).agg(\"mean\")\n",
        "#stds = df.groupby(['SPEAKER']).agg(\"std\")\n",
        "# for each speaker, calculate mean and standard deviation across all samples of that speaker\n",
        "\n",
        "means = df.groupby(['SPEAKER']).mean(numeric_only=True)\n",
        "\n",
        "stds = df.groupby(['SPEAKER']).std(numeric_only=True)\n",
        "\n",
        "#convert to numpy\n",
        "F1mean = means.F1[df.SPEAKER].to_numpy()\n",
        "F1std = stds.F1[df.SPEAKER].to_numpy()\n",
        "F2mean = means.F2[df.SPEAKER].to_numpy()\n",
        "F2std = stds.F2[df.SPEAKER].to_numpy()\n",
        "\n",
        "#normalise each speaker value of F1 and F2 to obtain a comon scale\n",
        "df[\"F1norm\"] = (df.F1 - F1mean) / F1std\n",
        "df[\"F2norm\"] = (df.F2 - F2mean) / F2std\n",
        "\n",
        "#\n",
        "print(df.describe())\n",
        "\n",
        "#\n",
        "plot_formants(df, f1=\"F1norm\", f2=\"F2norm\", axis_ranges=None)\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "test_set = df.sample(frac=0.05, random_state=0)\n",
        "print(test_set.describe())\n",
        "train_set = df.drop(test_set.index)\n",
        "print(train_set.describe())"
      ],
      "metadata": {
        "id": "yJ1v5amYPpC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(h) Train and evaluate a new classifier, as before, using the normalised formant data. What was the effect on the classification accuracy? Why?"
      ],
      "metadata": {
        "id": "x9jhqd5vTwZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(h)\n",
        "#ANSWER\n",
        "norm_vowel = KNeighborsClassifier(n_neighbors = 3)\n",
        "norm_vowel.fit(X= train_set[['F1norm','F2norm']], y = train_set[\"VOWEL\"])\n",
        "\n",
        "print(\"The train set accuracy is:\", norm_vowel.score(X = train_set[[\"F1norm\", \"F2norm\"]], y = train_set[[\"VOWEL\"]]), sep = \"\\t\")\n",
        "print(\"The test set accuracy is:\", norm_vowel.score(X = test_set[[\"F1norm\", \"F2norm\"]], y = test_set[[\"VOWEL\"]]), sep = \"\\t\")\n",
        "\n",
        "#by normalising the accuracy improved"
      ],
      "metadata": {
        "id": "PyNVmL3XVDne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) In this exercise we calculated the statistics for normalisation on all the data (train and test sets combined), is this problematic? What would the consequence be when calculating the generalisation error? When deploying this system, how would we perform this normalisation for a new (unseen) speaker?"
      ],
      "metadata": {
        "id": "qvwTrmD3WFCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(i)\n",
        "#ANSWER\n",
        "#can be problematic because it was normalised to test samples, generalisation to unknown samples could therefore yield lower accuracy and would need to preprocess test data to obtain similar speaker statistics to make sure a commn scale is kept"
      ],
      "metadata": {
        "id": "enZjMEUkWngw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(j) In this exercise we did not make use of a validation set, was it necessary? Why?"
      ],
      "metadata": {
        "id": "x8oG8adGWoCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(j)\n",
        "#ANSWER\n",
        "#The use of a validation set would have been necessary if we wanted to find better hyperparameters for the classifier\n",
        "#   (e.g. it is possible that using a larger number of neighbours could result in better generalisation error)"
      ],
      "metadata": {
        "id": "ga2JTudZWqse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}